{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LaxmiGunda/MovieChatbot-CaseStudyII/blob/main/Laxmivasavi_Gunda_MovieChatbotML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Movie chatbot"
      ],
      "metadata": {
        "id": "FTi_9pCYGehY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ## Laxmivasavi (Laxmi) Gunda\n",
        "- ## laxmi.gunda@gmail.com\n",
        "- ## Movie chatbot for interactive movie search\n"
      ],
      "metadata": {
        "id": "YMFXeNgRFp15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem**: The goal was to create a chatbot that can answer questions and provide recommendations about movies based on a dataset, going beyond simple keyword matching to understand the user's intent and the movie content.\n",
        "\n",
        "**Solution - RAG System Implementation**: The solution is built by building a Retrieval Augmented Generation (RAG) system in this notebook. The core idea of RAG is to augment a Language Model (LLM) with the ability to retrieve information from a custom knowledge base before generating a response.\n",
        "\n",
        "Here's how it is achieved:\n",
        "\n",
        "1. **Data Preparation**: started by loading the movie dataset, cleaning it by handling potential missing values and incorrect data types, and creating a comprehensive \"description\" for each movie.\n",
        "\n",
        "2. **Information Chunking and Indexing**: split the movie descriptions into smaller text chunks and extracted relevant metadata (Title, Genre, Year, etc.). Later then generated numerical representations (vector embeddings) for these chunks and stored them in a searchable **Vector Store (FAISS)**. Also created a **BM25 index** for keyword-based retrieval.\n",
        "\n",
        "3. **Hybrid Retrieval**: To answer user queries,  implemented a hybrid retrieval mechanism. This involves performing both a vector similarity search (finding semantically similar chunks) and a BM25 keyword search. The results from both methods are combined to get a more comprehensive set of relevant documents.\n",
        "\n",
        "4. **Language Model (LLM) and Prompting**: used a Language Model (LLM) (OpenAI's GPT-4o) as the core of the chatbot. I defined a Prompt Template to instruct the LLM on how to use the retrieved documents as context to answer user questions.\n",
        "\n",
        "5. **Agent and Tools**: Built a Langchain Agent powered by the LLM. The agent's role is to understand the user's natural language query and decide which tool to use. The agent is equipped with many Tools:\n",
        "    - A tool for direct filtering (list_movies_by_filters) to handle simple requests for lists of movies based purely on metadata (like genre or year), bypassing the LLM for efficiency.\n",
        "    - A tool for hybrid semantic and metadata search (semantic_search_movies_with_filters) to handle more complex queries requiring both understanding content and applying filters. This tool performs the combined vector and BM25 search and passes the results to the LLM for summarization.\n",
        "    - A tool to fetch the plot of a specific movie (get_movie_plot) from an external API.\n",
        "    - A tool to get the description of a specific movie (get_movie_description) from the dataset.\n",
        "\n",
        "6. **Session Memory** : I also added Conversation Memory to the agent so it can maintain context and respond naturally to follow-up questions within a conversation.\n",
        "\n",
        "7. **User Interface**: Used Gradio to create a simple web-based chat interface, allowing users to interact with the agent.\n",
        "\n",
        "\n",
        "By combining these components, the chatbot can understand user queries, retrieve relevant movie information from its knowledge base using different methods (keyword, semantic, hybrid, direct lookup), and use the LLM to generate helpful and contextually relevant responses. Made the system more robust by handling potential data errors and making the filtering process more explicit for the agent.\n"
      ],
      "metadata": {
        "id": "8WR8ziBTUDzQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usage\n",
        "\n",
        "**Note**: Anyone else who downloads and uses your notebook would need to:\n",
        "\n",
        "- **Have their own API keys**.\n",
        "- **Add their keys to their own Colab Secrets Manager under the same names (OPENAI_API_KEY, OMDB_API_KEY**)."
      ],
      "metadata": {
        "id": "D67O3aPeZJH5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRit5A9KCx20"
      },
      "outputs": [],
      "source": [
        "# Importing the OpenAI library to interact with OpenAI's API services.\n",
        "import openai\n",
        "\n",
        "# Import CSV module for reading and writing CSV files\n",
        "import csv\n",
        "\n",
        "# Import pandas for data manipulation and analysis\n",
        "import pandas as pd\n",
        "\n",
        "import langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "import warnings\n",
        "# Suppress all warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "YUrRhqQtIB6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import OpenAI API Key"
      ],
      "metadata": {
        "id": "eoWZI4PWObcW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7959e6dd"
      },
      "source": [
        "# Store your OpenAI API key\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Access the secret using the name you gave it in the secrets manager\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# You can then set it as an environment variable for the OpenAI library to use\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0c67af1"
      },
      "source": [
        "## Mount Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b0822a2"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the data set\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gEiLgmt953mL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XiPvpp1FxKb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd819e54-ca32-45e9-e407-be6e531045d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to load dataset from Google Drive: /content/drive/MyDrive/MovieChatbotData/IMDb_Dataset.csv\n",
            "Dataset loaded successfully from Google Drive.\n",
            "\n",
            "DataFrame shape: (3173, 10)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd # Ensure pandas is imported\n",
        "\n",
        "# Load the data from Google Drive first, with a fallback to /content/\n",
        "\n",
        "# Define the primary path in your Google Drive\n",
        "drive_file_path = '/content/drive/MyDrive/MovieChatbotData/IMDb_Dataset.csv'\n",
        "\n",
        "# Define the fallback path in the Colab environment\n",
        "content_file_path = '/content/IMDb_Dataset.csv'\n",
        "\n",
        "df = None # Initialize df to None\n",
        "\n",
        "try:\n",
        "    print(f\"Attempting to load dataset from Google Drive: {drive_file_path}\")\n",
        "    df = pd.read_csv(drive_file_path)\n",
        "    print(\"Dataset loaded successfully from Google Drive.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"File not found in Google Drive at {drive_file_path}. Attempting to load from Colab content directory: {content_file_path}\")\n",
        "    try:\n",
        "        df = pd.read_csv(content_file_path)\n",
        "        print(\"Dataset loaded successfully from Colab content directory.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found in Colab content directory at {content_file_path} either.\")\n",
        "        df = None # Ensure df is None if not found in both locations\n",
        "    except pd.errors.ParserError as e:\n",
        "        print(f\"Error: Could not parse the CSV file from Colab content directory: {e}\")\n",
        "        df = None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred loading from Colab content directory: {e}\")\n",
        "        df = None\n",
        "\n",
        "except pd.errors.ParserError as e:\n",
        "    print(f\"Error: Could not parse the CSV file from Google Drive: {e}\")\n",
        "    df = None\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred loading from Google Drive: {e}\")\n",
        "    df = None\n",
        "\n",
        "\n",
        "# Final check and print if df was loaded\n",
        "if df is not None:\n",
        "   print(f\"\\nDataFrame shape: {df.shape}\")\n",
        "   # Check if the DataFrame is empty after loading\n",
        "   if df.empty:\n",
        "       print(f\"Warning: The loaded DataFrame is empty.\")\n",
        "       df = None # Set df to None if it's empty even after loading\n",
        "else:\n",
        "   print(\"\\nDataFrame was not loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loading and preparation"
      ],
      "metadata": {
        "id": "jFzc0tia3-LU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MznuqvAmRx_g",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# View & Understand the data\n",
        "if df is not None:\n",
        "   display(df.head())\n",
        "else:\n",
        "   print(\"DataFrame not loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the features\n",
        "if df is not None:\n",
        "   display(df.info())"
      ],
      "metadata": {
        "id": "sTArtUb5FMeS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize the data frame with describe.\n",
        "if df is not None:\n",
        "  display(df.describe())"
      ],
      "metadata": {
        "id": "XO4jCTUlFVKY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the missing values across the columns\n",
        "if df is not None:\n",
        "  display(df.isnull().sum())"
      ],
      "metadata": {
        "id": "YM8EW3XqJrGM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5079f686"
      },
      "source": [
        "### Handle Missing Values\n",
        "\n",
        "Add code to handle potential missing values in the dataset, focusing on columns used for creating the movie description and metadata."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec2703d2"
      },
      "source": [
        "# Handle missing values in critical columns\n",
        "if df is not None:\n",
        "    # Define columns critical for the movie description and metadata\n",
        "    critical_columns = ['Title', 'IMDb Rating', 'Year', 'Certificates', 'Genre', 'Director', 'Star Cast', 'Duration (minutes)']\n",
        "\n",
        "    # Check for missing values in critical columns\n",
        "    print(\"Missing values in critical columns before handling:\")\n",
        "    display(df[critical_columns].isnull().sum())\n",
        "\n",
        "    initial_rows = len(df)\n",
        "    df.dropna(subset=critical_columns, inplace=True)\n",
        "    rows_dropped = initial_rows - len(df)\n",
        "\n",
        "    if rows_dropped > 0:\n",
        "        print(f\"\\nDropped {rows_dropped} rows with missing values in critical columns.\")\n",
        "    else:\n",
        "        print(\"\\nNo rows with missing values in critical columns found.\")\n",
        "\n",
        "    # Verify no missing values in critical columns after dropping\n",
        "    print(\"\\nMissing values in critical columns after handling:\")\n",
        "    display(df[critical_columns].isnull().sum())\n",
        "\n",
        "else:\n",
        "    print(\"DataFrame not loaded, cannot check or handle missing values.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "3b7ff251"
      },
      "source": [
        "# Get the unique directors and the display the top 10 directors\n",
        "\n",
        "if df is not None:\n",
        "  # Get the unique directors\n",
        "  unique_directors = df['Director'].unique()\n",
        "  print(\"Number of unique directors:\", len(unique_directors))\n",
        "  print(\"First 10 unique directors:\", unique_directors[:10])\n",
        "\n",
        "  # Count the number of movies per director (top 10)\n",
        "  director_counts = df['Director'].value_counts()\n",
        "  print(\"\\nTop 10 directors by number of movies:\")\n",
        "  display(director_counts.head(10))\n",
        "else:\n",
        "  print(\"DataFrame not loaded.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a43cbe0"
      },
      "source": [
        "### Handle Unexpected Data Types\n",
        "\n",
        "Add code to convert columns expected to be numeric to the correct data type, handling potential errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8aba496"
      },
      "source": [
        "# Handle unexpected data types in numeric columns\n",
        "if df is not None:\n",
        "    numeric_columns = ['Year', 'IMDb Rating', 'MetaScore', 'Duration (minutes)']\n",
        "    print(\"Attempting to convert numeric columns...\")\n",
        "\n",
        "    for col in numeric_columns:\n",
        "        if col in df.columns:\n",
        "            # Attempt to convert to numeric, coercing errors to NaN\n",
        "            initial_dtype = df[col].dtype\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "            if df[col].isnull().sum() > 0:\n",
        "                print(f\"Warning: Found non-numeric values in column '{col}' and converted them to NaN.\")\n",
        "            if df[col].dtype != initial_dtype:\n",
        "                print(f\"Converted column '{col}' from {initial_dtype} to {df[col].dtype}.\")\n",
        "            else:\n",
        "                 print(f\"Column '{col}' is already numeric or conversion did not change dtype ({df[col].dtype}).\")\n",
        "        else:\n",
        "            print(f\"Warning: Numeric column '{col}' not found in DataFrame.\")\n",
        "\n",
        "    print(\"\\nChecking for new missing values after type conversion:\")\n",
        "    display(df.isnull().sum())\n",
        "else:\n",
        "    print(\"DataFrame not loaded, cannot handle unexpected data types.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of movies by Genre\n",
        "\n",
        "genre_counts = df['Genre'].value_counts()\n",
        "display(genre_counts)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eislsUAgFgM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Observations:\n",
        "    - There are 3173 movies in this dataset.\n",
        "    - There are no null features in the dataset.\n",
        "    - Most movies have an IMDb rating between 6.4 and 7.5 (between the 25th and 75th percentiles) and a duration between 105 and 122 minutes.\n",
        "    - 50% is the median value and looking at the median and mean, the data seems to be mostly symmetrical.\n",
        "    - Most of the movies(868 out of 3173)  are belongs to \"Biography\" genre."
      ],
      "metadata": {
        "id": "HaX02sLkIO3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data visualization"
      ],
      "metadata": {
        "id": "UvmxomRWA4Bh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.boxplot(x=df['Duration (minutes)'])\n",
        "plt.title('Box Plot of Movie Duration (minutes)')\n",
        "plt.xlabel('Duration (minutes)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vibVV5Z8wRd-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Observations:\n",
        "    - Looking at the box plot for Duration or length of the movie, it looks like there are unusual short and long movies."
      ],
      "metadata": {
        "id": "ofQGE_bMxH8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bar plot to visualize between Genre and No. Of movies\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=genre_counts.index, y=genre_counts.values)\n",
        "plt.title(\"Distribution of Movies Across Genres\")\n",
        "plt.xlabel(\"Genre\")\n",
        "plt.ylabel(\"Number of Movies\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "agliEVzEc0D3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Observations\n",
        "\n",
        "  - The number of movies per genre was counted, showing a range from 868 for 'Biography' down to 1 for 'Reality-TV'."
      ],
      "metadata": {
        "id": "KQ4ucVvVdb3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text chunking"
      ],
      "metadata": {
        "id": "4IsLgbhu4Pp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add new column: Description"
      ],
      "metadata": {
        "id": "H024SByb4VyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create movie description for each movie from the details provided in the dataset\n",
        "\n",
        "df['description'] = (df['Title'] + \" | \" + df[\"Genre\"] + \" | \" +\n",
        "                     df['Year'].astype(str) + \" | Directed by \" + df['Director'] +\n",
        "                     \" | Starring \" + df['Star Cast'] + \" | IMDb Rating: \" +\n",
        "                     df['IMDb Rating'].astype(str))\n",
        "\n",
        "# Display the first few descriptions to check the result\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "3UXD8fTl-D8X",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ea958bc"
      },
      "source": [
        "### Define Save/Load Path in Google Drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "767cb6f7"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the directory path in your Google Drive to save the vector store\n",
        "# Replace 'YourProjectFolder' with the actual folder name in your Drive\n",
        "drive_save_path = '/content/drive/MyDrive/MovieChatbotData/faiss_index'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(drive_save_path):\n",
        "    os.makedirs(drive_save_path)\n",
        "    print(f\"Created directory: {drive_save_path}\")\n",
        "else:\n",
        "    print(f\"Directory already exists: {drive_save_path}\")\n",
        "\n",
        "print(f\"Vector store will be saved to and loaded from: {drive_save_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perform chunking and save to JSON file"
      ],
      "metadata": {
        "id": "SMg-NcUy450I"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ec59d01",
        "collapsed": true
      },
      "source": [
        "# Perform text chunking of the new \"Description\" feature and store to a JSON file if it doesn't exist.\n",
        "\n",
        "import json\n",
        "import os\n",
        "from langchain.text_splitter import CharacterTextSplitter # Import CharacterTextSplitter here\n",
        "\n",
        "file_path = '/content/drive/MyDrive/MovieChatbotData/IMDB_Data_chunks.json' # Changed to a Drive path\n",
        "chunk_data = [] # Use a list to store dictionaries with chunk text and metadata\n",
        "chunk_id_counter = 1 # Initialize a counter to start IDs from 1\n",
        "\n",
        "# Ensure the directory for the JSON file exists\n",
        "json_dir = os.path.dirname(file_path)\n",
        "if not os.path.exists(json_dir):\n",
        "    os.makedirs(json_dir)\n",
        "    print(f\"Created directory for JSON file: {json_dir}\")\n",
        "\n",
        "# Initialize the text splitter OUTSIDE the if/else block\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\",\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "print(\"Text splitter initialized.\") # Added print statement to confirm initialization\n",
        "\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    print(f\"Loading chunks from {file_path}\")\n",
        "    # Load the structured JSON data\n",
        "    with open(file_path, 'r') as f:\n",
        "        chunk_data = json.load(f)\n",
        "    # If loading existing data, find the max ID to continue numbering\n",
        "    if chunk_data:\n",
        "        # Assuming IDs are sequential, find the highest existing ID\n",
        "        # Use max([item.get('id', 0) for item in chunk_data]) to handle potential missing IDs and ensure we start after the highest existing one\n",
        "        max_id = max([item.get('id', 0) for item in chunk_data])\n",
        "        chunk_id_counter = max_id + 1\n",
        "    else:\n",
        "        # If file exists but is empty, start counting from 1\n",
        "        chunk_id_counter = 1\n",
        "else:\n",
        "    print(\"Creating chunks and saving to file...\")\n",
        "    # The text splitter is already initialized above\n",
        "    print(text_splitter) # Keep this print if you want to see the splitter config\n",
        "\n",
        "\n",
        "    # Chunk the descriptions and include metadata\n",
        "    if df is not None:\n",
        "        for index, row in df.iterrows():\n",
        "            movie_chunks = text_splitter.split_text(row['description'])\n",
        "            for i, chunk in enumerate(movie_chunks):\n",
        "                # Create a dictionary for each chunk including text, metadata, and a unique ID\n",
        "                chunk_info = {\n",
        "                    \"id\": chunk_id_counter, # Add a unique ID\n",
        "                    \"text\": chunk,\n",
        "                    \"metadata\": {\n",
        "                        \"Title\": row['Title'],\n",
        "                        \"Year\": row['Year'],\n",
        "                        \"Certificates\": row['Certificates'],\n",
        "                        \"Genre\": row['Genre'],\n",
        "                        \"Director\": row['Director'],\n",
        "                        \"Star Cast\": row['Star Cast'],\n",
        "                        \"IMDb Rating\": row['IMDb Rating'],\n",
        "                        \"Duration (minutes)\": row['Duration (minutes)'],\n",
        "                        \"chunk_index\": i # Keep track of chunk order within the movie\n",
        "                    }\n",
        "                }\n",
        "                chunk_data.append(chunk_info)\n",
        "                chunk_id_counter += 1 # Increment the counter for the next chunk\n",
        "    else:\n",
        "        print(\"DataFrame not loaded, cannot create chunks.\")\n",
        "\n",
        "\n",
        "    # Save the structured chunk data to the JSON file\n",
        "    if chunk_data: # Only save if there's data to save\n",
        "        with open(file_path, 'w') as f:\n",
        "            json.dump(chunk_data, f, indent=4)\n",
        "\n",
        "        print(f\"Chunks with ID and metadata successfully saved to {file_path}\")\n",
        "    else:\n",
        "        print(\"No chunk data generated to save.\")\n",
        "\n",
        "\n",
        "# Display the first few chunks to verify the structure\n",
        "print(\"\\nFirst 5 chunks with ID and metadata:\")\n",
        "for i, item in enumerate(chunk_data[:5]):\n",
        "    print(f\"Chunk {i+1}:\")\n",
        "    print(f\"  ID: {item.get('id', 'N/A')}\") # Use .get for safety in case 'id' is missing somehow\n",
        "    print(f\"  Text: {item['text']}\")\n",
        "    print(f\"  Metadata: {item['metadata']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "519e43e3"
      },
      "source": [
        "### Implement BM25 Retrieval\n",
        "\n",
        "Add code to implement BM25 retrieval using the `rank_bm25` library. This is a keyword-based retrieval method that can complement vector search."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install rank_bm25"
      ],
      "metadata": {
        "id": "bZMtxGWn7BlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install nltk"
      ],
      "metadata": {
        "id": "IVGbjUec-zF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b87276bd"
      },
      "source": [
        "### Download NLTK Data\n",
        "\n",
        "Explicitly download necessary NLTK data resources like 'punkt' and 'stopwords'. This should be run before using NLTK resources for tokenization or preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2be0dfec"
      },
      "source": [
        "import nltk\n",
        "\n",
        "# Download 'punkt' tokenizer data\n",
        "try:\n",
        "    nltk.download('punkt')\n",
        "    print(\"NLTK 'punkt' data downloaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading NLTK 'punkt' data: {e}\")\n",
        "\n",
        "# Download 'stopwords' corpus data\n",
        "try:\n",
        "    nltk.download('stopwords')\n",
        "    print(\"NLTK 'stopwords' data downloaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading NLTK 'stopwords' data: {e}\")\n",
        "\n",
        "try:\n",
        "    nltk.download('punkt_tab')\n",
        "    print(\"NLTK 'punkt_tab' data downloaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading NLTK 'punkt_tab' data: {e}\")\n",
        "\n",
        "print(\"\\nNLTK data download attempts complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "723c9512"
      },
      "source": [
        "### Create BM25 Index\n",
        "\n",
        "Create the BM25 index from the processed text chunks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffffe56b4"
      },
      "source": [
        "# Implement BM25 Retrieval\n",
        "from rank_bm25 import BM25Okapi\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "\n",
        "# --- Text Preprocessing for BM25 ---\n",
        "# BM25 works best with tokenized and potentially cleaned text (e.g., removed stopwords, punctuation)\n",
        "# Use the text content from your chunk_data (ensure chunk_data is available from cell 0ec59d01)\n",
        "\n",
        "if 'chunk_data' not in globals() or not chunk_data:\n",
        "    print(\"Chunk data not available. Please run the chunking cell (0ec59d01) first.\")\n",
        "else:\n",
        "    print(\"Preparing text data for BM25...\")\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Define the preprocessing function (should be accessible globally or within the tool)\n",
        "    def preprocess_text(text):\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "        # Remove punctuation\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        # Tokenize\n",
        "        text = nltk.word_tokenize(text) # Tokenize after punctuation removal\n",
        "        # Remove stopwords and non-alphabetic tokens\n",
        "        tokens = [word for word in text if word.isalpha() and word not in stop_words]\n",
        "        return tokens\n",
        "\n",
        "    # Preprocess and tokenize each chunk\n",
        "    # Using the 'text' field from the chunk_data dictionaries\n",
        "    tokenized_corpus = [preprocess_text(item['text']) for item in chunk_data]\n",
        "\n",
        "    # --- Create BM25 Index ---\n",
        "    print(\"Creating BM25 index...\")\n",
        "    # Initialize BM25 with the tokenized corpus\n",
        "    bm25 = BM25Okapi(tokenized_corpus)\n",
        "    print(\"BM25 index created.\")\n",
        "\n",
        "    print(\"\\nBM25 setup complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector embeddings for chunks"
      ],
      "metadata": {
        "id": "ery6W2p84jSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install sentence-transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NgzXOCFX5Grv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create vector embeddings"
      ],
      "metadata": {
        "id": "Y0vTYOoBQS4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create embeddings for the chunks\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np # Import numpy for checking shape\n",
        "\n",
        "# Load Sentence Transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Extract just the text content from the chunk_data dictionaries\n",
        "chunk_texts = [item['text'] for item in chunk_data]\n",
        "\n",
        "# Create vector embeddings for chunks using the extracted text\n",
        "semanticEmbeddings = model.encode(chunk_texts)\n",
        "\n",
        "# Display the shape of the embeddings\n",
        "display(semanticEmbeddings.shape)"
      ],
      "metadata": {
        "id": "hUutqmZF-jf4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the shape of the semantic embeddings and the first 10 embeddings\n",
        "display(semanticEmbeddings.shape)\n",
        "display(semanticEmbeddings[:10])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rBaTxIFQBY0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use Vector store to store embeddings"
      ],
      "metadata": {
        "id": "lF40xf-q4s0T"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "67e53823"
      },
      "source": [
        "%pip install faiss-cpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install langchain-community"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NgnHZ_yRo5FP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Store embeddings to FAISS"
      ],
      "metadata": {
        "id": "ql65XUubQayE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSlO9vblWGDt"
      },
      "outputs": [],
      "source": [
        "# Store embeddings to FAISS\n",
        "import faiss\n",
        "import numpy as np\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.schema import Document # Import Document\n",
        "import os # Import os for path checking\n",
        "\n",
        "embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "vector_store = None # Initialize vector_store to None\n",
        "\n",
        "# Define the path where the FAISS index is saved (same as defined earlier)\n",
        "# Ensure drive_save_path is defined in a previous cell (e.g., cell 767cb6f7)\n",
        "# drive_save_path = '/content/drive/MyDrive/MovieChatbotData/faiss_index' # Assuming this is defined\n",
        "\n",
        "# Check if the FAISS index already exists in Google Drive\n",
        "if drive_save_path and os.path.exists(drive_save_path) and os.path.exists(os.path.join(drive_save_path, 'index.faiss')):\n",
        "    print(f\"Loading existing FAISS index from {drive_save_path}\")\n",
        "    try:\n",
        "        # Load the vector store from the specified path\n",
        "        vector_store = FAISS.load_local(\n",
        "            drive_save_path,\n",
        "            embedding_function,\n",
        "            allow_dangerous_deserialization=True # Set to True if loading a FAISS index from a trusted source\n",
        "        )\n",
        "        print(\"FAISS index successfully loaded.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading FAISS index: {e}\")\n",
        "        # If loading fails, you might want to proceed with creating a new index\n",
        "        print(\"Proceeding with creating a new FAISS index.\")\n",
        "\n",
        "# If the vector store was not loaded (because it didn't exist or loading failed), create a new one\n",
        "if vector_store is None:\n",
        "    print(\"Creating a new FAISS index.\")\n",
        "\n",
        "    # Create a list of Document objects from the chunks, including metadata\n",
        "    # Ensure 'df' and 'text_splitter' are available from previous cells (e.g., 0XiPvpp1FxKb and 0ec59d01)\n",
        "    documents = []\n",
        "    if df is not None and 'description' in df.columns and text_splitter is not None:\n",
        "        for index, row in df.iterrows():\n",
        "            movie_chunks = text_splitter.split_text(row['description'])\n",
        "            for i, chunk in enumerate(movie_chunks):\n",
        "                metadata = {\n",
        "                    \"Title\": row['Title'],\n",
        "                    \"Year\": row['Year'],\n",
        "                    \"Certificates\": row['Certificates'],\n",
        "                    \"Genre\": row['Genre'],\n",
        "                    \"Director\": row['Director'],\n",
        "                    \"Star Cast\": row['Star Cast'],\n",
        "                    \"IMDb Rating\": row['IMDb Rating'],\n",
        "                    \"Duration (minutes)\": row['Duration (minutes)'],\n",
        "                    \"chunk_index\": i\n",
        "                }\n",
        "                documents.append(Document(page_content=chunk, metadata=metadata))\n",
        "    elif df is None:\n",
        "         print(\"DataFrame 'df' not loaded, cannot create documents for FAISS.\")\n",
        "    elif 'description' not in df.columns:\n",
        "         print(\"'description' column not found in DataFrame, cannot create documents for FAISS.\")\n",
        "    elif text_splitter is None:\n",
        "         print(\"'text_splitter' not initialized, cannot create documents for FAISS.\")\n",
        "\n",
        "\n",
        "    if documents:\n",
        "        try:\n",
        "            vector_store = FAISS.from_documents(\n",
        "                documents=documents,\n",
        "                embedding=embedding_function\n",
        "            )\n",
        "            print(\"Successfully created Langchain FAISS vector store from documents with metadata.\")\n",
        "\n",
        "            # Save the newly created vector store to Google Drive\n",
        "            if drive_save_path: # Check if the save path is defined\n",
        "                try:\n",
        "                    vector_store.save_local(drive_save_path)\n",
        "                    print(f\"FAISS index successfully saved to {drive_save_path}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error saving FAISS index to {drive_save_path}: {e}\")\n",
        "            else:\n",
        "                print(\"drive_save_path is not defined. Cannot save FAISS index.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating FAISS index from documents: {e}\")\n",
        "            vector_store = None # Ensure vector_store is None if creation fails\n",
        "    else:\n",
        "        print(\"No documents available to create FAISS vector store.\")\n",
        "        vector_store = None # Ensure vector_store is None if no documents\n",
        "\n",
        "\n",
        "# After this cell runs, 'vector_store' will either contain the loaded index or a newly created one (or None if errors occurred)\n",
        "if vector_store:\n",
        "    print(\"\\nFAISS vector store is ready to use.\")\n",
        "else:\n",
        "    print(\"\\nFAISS vector store is NOT available.\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "388c4ca2"
      },
      "source": [
        "if vector_store is not None:\n",
        "    # Access the underlying FAISS index and its ntotal attribute\n",
        "    faiss_index = vector_store.index\n",
        "    print(f\"The size of the vector store (number of vectors) is: {faiss_index.ntotal}\")\n",
        "else:\n",
        "    print(\"Vector store has not been loaded yet.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define LLM"
      ],
      "metadata": {
        "id": "o6LUqa1T5Eee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install langchain_openai"
      ],
      "metadata": {
        "collapsed": true,
        "id": "O03DQqVPu111"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import GPT4.0"
      ],
      "metadata": {
        "id": "a3i-XL11RMSF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dJXzILCXStu",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Create the llm model - Using OpenAI GPT-4\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize the GPT-4 model\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0) # temperature=0 for more deterministic responses\n",
        "\n",
        "print(\"OpenAI GPT-4o model is now available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add a new prompt template"
      ],
      "metadata": {
        "id": "D3W4Ei4a9CdT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a278d4c6"
      },
      "source": [
        "# Create the prompt template\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "movie_search_template = \"\"\"You are a movie recommendation assistant.\n",
        "Given the following movie information:\n",
        "{context}\n",
        "\n",
        "Answer the user's question about movies. If you don't know the answer, don't try to make up an answer.\n",
        "\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "movie_search_prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=movie_search_template,\n",
        ")\n",
        "\n",
        "print(f\"Movie search prompt: {movie_search_prompt}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create retrieval chain"
      ],
      "metadata": {
        "id": "EaSEz2_29Jpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the document processing chain\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate # Import PromptTemplate\n",
        "\n",
        "# Initialize the document processing chain with the new custom movie_search_prompt\n",
        "\n",
        "# Configure the retriever to potentially use metadata filters\n",
        "# For example, to filter by genre (this is a static example,\n",
        "# you'll likely make this dynamic with the agent later)\n",
        "# search_kwargs = {'filter': {'Genre': 'Action'}} # Example filter\n",
        "# search_kwargs = {'k': 5} # Example: retrieve top 5 documents\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vector_store.as_retriever(\n",
        "        search_kwargs={'k': 5}\n",
        "    ),\n",
        "    chain_type_kwargs={\"prompt\": movie_search_prompt} # Passing movie_search_prompt here\n",
        ")\n",
        "\n",
        "print(\"Retrieval chain created with configured retriever!\")"
      ],
      "metadata": {
        "id": "jAiIUA1T_FPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25fdd2a5"
      },
      "source": [
        "# Test to run a sample query and check the output\n",
        "\n",
        "user_query = \"What is a good action movie?\"\n",
        "# Use the SentenceTransformer model (loaded in cell hUutqmZF-jf4, variable is still 'model') to encode the query\n",
        "# If you had renamed the SentenceTransformer model, you would use that variable name here.\n",
        "query_embedding = model.encode(user_query)\n",
        "\n",
        "display(query_embedding.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the user query embeddings for testing purposes\n",
        "result = qa_chain.invoke(user_query)\n",
        "display(result)"
      ],
      "metadata": {
        "id": "pFS-hXIlN71_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create User interface for intermediate check"
      ],
      "metadata": {
        "id": "OK5CM13P9Pv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Test the functionality using a Gradio UI (intermediate check)\n",
        "import gradio as gr\n",
        "\n",
        "#The output of qa_chain.invoke() is a dictionary that includes the\n",
        "# original query and the generated answer, often under a key like 'result'.\n",
        "# So to get result, explicitly pass the 'result' key\n",
        "\n",
        "#Note: to debug any errors with Gradio in Colab, set debug=True in launch below..\n",
        "\n",
        "gr.ChatInterface(lambda message, history: qa_chain.invoke(message)['result']).launch(share=True)"
      ],
      "metadata": {
        "id": "cy7cQ7KG_kSG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define and initialize agent"
      ],
      "metadata": {
        "id": "ZmDx3G-t2vDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent, AgentType\n",
        "from langchain.tools import tool"
      ],
      "metadata": {
        "id": "TAcRlvwmzzpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define agent tools"
      ],
      "metadata": {
        "id": "9GSPdrAXPEq2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "9rlYRLXl0bft"
      },
      "source": [
        "tools = [] # Initialize or clear the tools list here\n",
        "\n",
        "from langchain.tools import tool\n",
        "# from pydantic import BaseModel, Field # Already imported in other tool cells\n",
        "\n",
        "@tool\n",
        "def get_movie_description(movie_title: str) -> str:\n",
        "  \"\"\"\n",
        "  Searches the dataset for a movie by its exact title and returns its full description from the dataset metadata.\n",
        "  Use this tool when the user asks for general information about a specific movie.\n",
        "  The input MUST be the exact movie title as a string for the 'movie_title' parameter.\n",
        "  \"\"\"\n",
        "  if df is None:\n",
        "      return \"Error: DataFrame is not loaded.\"\n",
        "  try:\n",
        "    # Assuming df is available globally\n",
        "    # Use case=False for case-insensitive matching in title search\n",
        "    matching_movies = df[df['Title'].str.contains(movie_title, case=False, na=False)]\n",
        "\n",
        "    if not matching_movies.empty:\n",
        "        # Return the description of the first matching movie (can be refined for multiple matches)\n",
        "        return matching_movies['description'].values[0]\n",
        "    else:\n",
        "        return f\"Sorry, I could not find a movie with the title '{movie_title}' in the dataset.\"\n",
        "  except Exception as e:\n",
        "    return f\"An error occurred while searching for the movie description: {e}\"\n",
        "\n",
        "tools.append(get_movie_description)\n",
        "\n",
        "print(\"Added a new tool 'get_movie_description'\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c8a5431"
      },
      "source": [
        "### Add a new tool for Direct Filtering\n",
        "\n",
        "This tool uses pandas to directly filter the DataFrame based on metadata, bypassing the vector store and LLM for filter-only queries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b74587a9"
      },
      "source": [
        "from langchain.tools import tool\n",
        "import pandas as pd # Ensure pandas is imported\n",
        "\n",
        "@tool\n",
        "def list_movies_by_filters(filter_criteria: dict) -> str: # Keep the type hint for clarity\n",
        "    \"\"\"\n",
        "    Filters the movie DataFrame directly based on metadata criteria and lists matching titles.\n",
        "    Use this tool ONLY for queries that can be answered by listing movies based purely on explicit metadata filters (like Genre, Year, Director, or Rating ranges), without needing semantic search or summarization.\n",
        "    The input MUST be a dictionary for 'filter_criteria'.\n",
        "\n",
        "    Example filter_criteria:\n",
        "    {'Genre': 'Action', 'Year': 2022}\n",
        "    {'Director': 'Christopher Nolan'}\n",
        "    {'IMDb Rating': {'$gte': 8.0}} # Example of range filter for numeric columns\n",
        "\n",
        "    Returns a comma-separated string of matching movie titles.\n",
        "    \"\"\"\n",
        "    if df is None:\n",
        "        return \"Error: DataFrame is not loaded.\"\n",
        "\n",
        "    filtered_df = df.copy() # Work on a copy to avoid modifying the original df\n",
        "\n",
        "    try:\n",
        "        # Iterate through the filter_criteria dictionary\n",
        "        for key, value in filter_criteria.items():\n",
        "            if key in filtered_df.columns:\n",
        "                if isinstance(value, dict):\n",
        "                    # Handle range filters (e.g., {'$gte': 2010, '$lte': 2019})\n",
        "                    if '$gte' in value and '$lte' in value:\n",
        "                        filtered_df = filtered_df[\n",
        "                            (filtered_df[key] >= value['$gte']) &\n",
        "                            (filtered_df[key] <= value['$lte'])\n",
        "                        ]\n",
        "                    elif '$gte' in value:\n",
        "                        filtered_df = filtered_df[filtered_df[key] >= value['$gte']]\n",
        "                    elif '$lte' in value:\n",
        "                        filtered_df = filtered_df[filtered_df[key] <= value['$lte']]\n",
        "                    # Add more range operators if needed ($gt, $lt)\n",
        "                    else:\n",
        "                         return f\"Unsupported range filter format for key '{key}'.\"\n",
        "                else:\n",
        "                    # Handle exact match filtering (case-insensitive for strings where appropriate)\n",
        "                    if filtered_df[key].dtype == 'object':\n",
        "                         # Use .str.contains with case=False for string columns\n",
        "                         filtered_df = filtered_df[filtered_df[key].str.contains(str(value), case=False, na=False)]\n",
        "                    else:\n",
        "                         # Direct equality for numeric or other types\n",
        "                         filtered_df = filtered_df[filtered_df[key] == value]\n",
        "            else:\n",
        "                return f\"Error: Filter key '{key}' not found in DataFrame columns.\"\n",
        "\n",
        "        if not filtered_df.empty:\n",
        "            # Return a formatted string of titles\n",
        "            movie_titles = filtered_df['Title'].tolist()\n",
        "            return \"Movies matching your criteria: \" + \", \".join(movie_titles[:20]) + (\", ...\" if len(movie_titles) > 20 else \"\")\n",
        "        else:\n",
        "            return \"Sorry, I could not find any movies matching your criteria.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred during direct filtering: {e}\"\n",
        "\n",
        "print(\"list_movies_by_filters tool defined without explicit input schema.\")\n",
        "\n",
        "# Add this new tool to your existing tools list\n",
        "try:\n",
        "    # Remove the old tool definition if it exists to avoid duplicates\n",
        "    tools = [t for t in tools if t.name != 'list_movies_by_filters']\n",
        "    tools.append(list_movies_by_filters)\n",
        "    print(\"list_movies_by_filters tool added to the tools list.\")\n",
        "except NameError:\n",
        "    print(\"Warning: 'tools' list not found. Please ensure the 'tools' list from cell 9rlYRLxl0bft is defined before adding this tool.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "185ac47e"
      },
      "source": [
        "### Add a new tool for Filtered Movie Search\n",
        "\n",
        "This tool allows the agent to perform vector searches with metadata filters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a5293de"
      },
      "source": [
        "from langchain.tools import tool\n",
        "from langchain.chains import RetrievalQA # Import RetrievalQA if you want the tool to run a QA chain internally\n",
        "from pydantic import BaseModel, Field # Import BaseModel and Field\n",
        "from langchain.schema import Document # Import Document object\n",
        "\n",
        "# Assume bm25 index and preprocess_text function are available from cell f0a4c179\n",
        "# Assume chunk_data is available from cell 0ec59d01\n",
        "\n",
        "\n",
        "# Define a Pydantic model for the input of the semantic_search_movies_with_filters tool\n",
        "class SemanticSearchInput(BaseModel):\n",
        "    query: str = Field(..., description=\"The user's natural language query about the movie content or theme.\")\n",
        "    filters: dict = Field(default=None, description=\"Optional: A dictionary where keys are metadata fields (e.g., 'Genre', 'Year', 'Director', 'IMDb Rating') and values are the desired filter values or range queries. Use this to refine semantic search results based on metadata.\")\n",
        "\n",
        "@tool(args_schema=SemanticSearchInput) # Link the Pydantic model to the tool\n",
        "def semantic_search_movies_with_filters(query: str, filters: dict = None) -> str:\n",
        "    \"\"\"\n",
        "    Performs a hybrid semantic (vector) and keyword (BM25) search for movies,\n",
        "    applies metadata filters, and uses an LLM to answer the query.\n",
        "    Use this tool for queries that involve semantic understanding, keyword matching,\n",
        "    and/or specific criteria (like Genre, Year, Rating).\n",
        "    The input MUST include a 'query' string and can optionally include a 'filters' dictionary.\n",
        "\n",
        "    Example filters:\n",
        "    {'Genre': 'Action'}\n",
        "    {'Year': 2022}\n",
        "    {'IMDb Rating': {'$gte': 7.5}} # Example using MongoDB-like query for rating >= 7.5\n",
        "    \"\"\"\n",
        "    if vector_store is None:\n",
        "        return \"Error: Vector store is not available.\"\n",
        "    # Ensure bm25 index, preprocess_text function, and chunk_data are available globally\n",
        "    if 'bm25' not in globals() or 'preprocess_text' not in globals():\n",
        "        return \"Error: BM25 index or preprocess_text function not available. Please run the BM25 setup cell (f0a4c179 or fffe56b4).\"\n",
        "    if 'chunk_data' not in globals() or not chunk_data:\n",
        "         return \"Error: Chunk data not available. Please run the chunking cell (0ec59d01).\"\n",
        "\n",
        "\n",
        "    try:\n",
        "        # --- 1. Perform Vector Search ---\n",
        "        # Configure the base retriever with filters if provided\n",
        "        vector_search_k = 10 # Retrieve more documents initially before combining\n",
        "        retriever_kwargs = {'k': vector_search_k}\n",
        "        if filters:\n",
        "            retriever_kwargs['filter'] = filters\n",
        "            print(f\"Applying filters to vector search: {filters}\") # For debugging\n",
        "\n",
        "        base_retriever = vector_store.as_retriever(search_kwargs=retriever_kwargs)\n",
        "        vector_docs = base_retriever.get_relevant_documents(query)\n",
        "        print(f\"Vector search retrieved {len(vector_docs)} documents.\")\n",
        "\n",
        "        # --- 2. Perform BM25 Search ---\n",
        "        # Note: BM25 filtering by metadata is not directly supported like in Vector Store/FAISS.\n",
        "        # BM25 searches the entire corpus based on keywords.\n",
        "\n",
        "        tokenized_query = preprocess_text(query)\n",
        "\n",
        "        # Get top documents by BM25 score\n",
        "        bm25_search_k = 10 # Retrieve top 10 documents from BM25\n",
        "\n",
        "        # *** MODIFIED BM25 RETRIEVAL ***\n",
        "        # Get the indices of the top N documents based on BM25 scores\n",
        "        bm25_top_indices = bm25.get_top_n(tokenized_query, n=bm25_search_k) # Get indices from the BM25 object\n",
        "\n",
        "        # Retrieve the original Document objects from chunk_data using the indices\n",
        "        bm25_docs = []\n",
        "        for i in bm25_top_indices:\n",
        "            # Check if the index is valid for chunk_data\n",
        "            if 0 <= i < len(chunk_data):\n",
        "                item = chunk_data[i]\n",
        "                 # Ensure the item has 'text' and 'metadata' keys as expected\n",
        "                if isinstance(item, dict) and 'text' in item and 'metadata' in item:\n",
        "                     bm25_docs.append(Document(page_content=item['text'], metadata=item['metadata']))\n",
        "                else:\n",
        "                     print(f\"Warning: BM25 index {i} returned an item not in expected dictionary format.\")\n",
        "            else:\n",
        "                 print(f\"Warning: BM25 index {i} is out of bounds for chunk_data.\")\n",
        "\n",
        "\n",
        "        print(f\"BM25 search retrieved {len(bm25_docs)} documents.\")\n",
        "\n",
        "\n",
        "        # --- 3. Combine Results (Simple Union) ---\n",
        "        # Combine documents from both searches. Remove duplicates based on page_content and metadata.\n",
        "        unique_docs = {}\n",
        "        for doc in vector_docs + bm25_docs:\n",
        "            # Create a unique key based on content and metadata\n",
        "            # Use a tuple of sorted metadata items for consistent hashing\n",
        "            metadata_key = tuple(sorted(doc.metadata.items()))\n",
        "            doc_key = (doc.page_content, metadata_key)\n",
        "            unique_docs[doc_key] = doc\n",
        "\n",
        "        combined_docs = list(unique_docs.values())\n",
        "        print(f\"Combined retrieval resulted in {len(combined_docs)} unique documents.\")\n",
        "\n",
        "        # Optionally, apply reranking here on the combined documents if needed\n",
        "        # or just pass the combined results to the LLM.\n",
        "\n",
        "        # --- 4. Pass Combined Results to LLM via QA Chain ---\n",
        "        # Ensure 'llm' and 'movie_search_prompt' are available\n",
        "        if llm is None:\n",
        "             return \"Error: Language model (LLM) is not available.\"\n",
        "        if movie_search_prompt is None:\n",
        "             return \"Error: Movie search prompt is not available.\"\n",
        "\n",
        "        # Use a simple retriever to pass the combined docs to the QA chain\n",
        "        # Create a retriever from the list of documents\n",
        "        from langchain.schema import BaseRetriever as LangchainBaseRetriever\n",
        "        from typing import List\n",
        "        from langchain.callbacks.manager import CallbackManagerForRetrieverRun\n",
        "\n",
        "        class ListBasedRetriever(LangchainBaseRetriever):\n",
        "            docs: List[Document]\n",
        "\n",
        "            def _get_relevant_documents(\n",
        "                self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
        "            ) -> List[Document]:\n",
        "                return self.docs\n",
        "\n",
        "\n",
        "        # Create an instance of the ListBasedRetriever with the combined documents\n",
        "        combined_retriever = ListBasedRetriever(docs=combined_docs)\n",
        "\n",
        "\n",
        "        qa_chain_in_tool = RetrievalQA.from_chain_type(\n",
        "            llm=llm,\n",
        "            chain_type=\"stuff\", # Stuff all docs into the context\n",
        "            retriever=combined_retriever, # Use the retriever with combined docs\n",
        "            chain_type_kwargs={\"prompt\": movie_search_prompt}\n",
        "        )\n",
        "\n",
        "        # Invoke the QA chain with the user's query\n",
        "        result = qa_chain_in_tool.invoke({\"query\": query})\n",
        "\n",
        "        # The result from the QA chain is a dictionary, return the 'output' key\n",
        "        return result.get('output', 'Could not generate a helpful answer based on the hybrid search results.')\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred during hybrid search: {e}\"\n",
        "\n",
        "print(\"semantic_search_movies_with_filters tool defined with hybrid search.\")\n",
        "\n",
        "# Add this tool to your existing tools list (assuming 'tools' list is defined elsewhere)\n",
        "# Ensure the 'tools' list is accessible and defined in cell 9rlYRLxl0bft\n",
        "try:\n",
        "    # Remove the old tool definition if it exists to avoid duplicates\n",
        "    tools = [t for t in tools if t.name != 'semantic_search_movies_with_filters']\n",
        "    tools.append(semantic_search_movies_with_filters)\n",
        "    print(\"semantic_search_movies_with_filters tool added to the tools list.\")\n",
        "except NameError:\n",
        "    print(\"Warning: 'tools' list not found. Please ensure the 'tools' list from cell 9rlYRLxl0bft is defined before adding this tool.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12fb1e79"
      },
      "source": [
        "### Add a new tool to fetch Movie Plot\n",
        "\n",
        "This tool will attempt to fetch the movie plot from an external API (like OMDb). Obtained a new key for OMDB and stored in Keys."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dbda80c"
      },
      "source": [
        "import requests\n",
        "from google.colab import userdata\n",
        "from langchain.tools import tool\n",
        "import os\n",
        "from pydantic import BaseModel, Field # Import BaseModel and Field\n",
        "\n",
        "# Define a Pydantic model for the input of the get_movie_plot tool\n",
        "class GetMoviePlotInput(BaseModel):\n",
        "    movie_title: str = Field(..., description=\"The exact title of the movie for which to fetch the plot.\")\n",
        "\n",
        "\n",
        "# --- IMPORTANT ---\n",
        "# You will need an API key for an external movie database like OMDb (www.omdbapi.com).\n",
        "# Get your API key and add it to Colab Secrets under the name 'OMDB_API_KEY'.\n",
        "OMDB_API_KEY = userdata.get('OMDB_API_KEY')\n",
        "\n",
        "@tool(args_schema=GetMoviePlotInput) # Link the Pydantic model to the tool\n",
        "def get_movie_plot(movie_title: str) -> str:\n",
        "  \"\"\"\n",
        "  Fetches the plot summary for a specific movie given its exact title using an external API (like OMDb).\n",
        "  Use this tool when the user explicitly asks for the plot of a named movie.\n",
        "  Requires an API key for the external service.\n",
        "  The input MUST be the exact movie title as a string for the 'movie_title' parameter.\n",
        "  \"\"\"\n",
        "  if not OMDB_API_KEY:\n",
        "      return \"Error: OMDB_API_KEY not found in Colab Secrets. Please add it.\"\n",
        "\n",
        "  base_url = \"http://www.omdbapi.com/\"\n",
        "  params = {\n",
        "      \"t\": movie_title,\n",
        "      \"apikey\": OMDB_API_KEY,\n",
        "      \"plot\": \"full\" # Request the full plot\n",
        "  }\n",
        "\n",
        "  try:\n",
        "    response = requests.get(base_url, params=params)\n",
        "    response.raise_for_status() # Raise an exception for bad status codes\n",
        "    data = response.json()\n",
        "\n",
        "    if data.get(\"Response\") == \"True\":\n",
        "      return data.get(\"Plot\", \"Plot information not available.\")\n",
        "    else:\n",
        "      return data.get(\"Error\", f\"Could not find plot information for '{movie_title}'.\")\n",
        "\n",
        "  except requests.exceptions.RequestException as e:\n",
        "    return f\"Error fetching movie plot from API: {e}\"\n",
        "  except Exception as e:\n",
        "    return f\"An unexpected error occurred: {e}\"\n",
        "\n",
        "# Add this new tool to your existing tools list (assuming 'tools' list is defined elsewhere)\n",
        "# Ensure the 'tools' list is accessible in this cell's scope if it's not global\n",
        "try:\n",
        "    tools.append(get_movie_plot)\n",
        "    print(\"get_movie_plot tool added to the tools list.\")\n",
        "except NameError:\n",
        "    print(\"Warning: 'tools' list not found. Please ensure the 'tools' list from cell 9rlYRLXl0bft is defined before adding this tool.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize the agent (Orchestration)"
      ],
      "metadata": {
        "id": "7SDF9R7R3GLm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFhn7mcf_ziw"
      },
      "source": [
        "# Initialize the agent with memory\n",
        "# Ensure 'tools' list is updated with the new tools\n",
        "# Ensure 'llm' is defined\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory # Import memory component\n",
        "from langchain.agents import initialize_agent, AgentType # Ensure these are imported\n",
        "\n",
        "# Add a prefix to guide the agent's behavior, encouraging it to use the appropriate tool\n",
        "# This prefix is still relevant for guiding the agent's overall thought process\n",
        "agent_prefix = \"\"\"You are a helpful movie recommendation assistant.\n",
        "You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "When a user asks a question about movies, first think step-by-step:\n",
        "1. Identify the user's intent. Are they asking for a list of movies based purely on filters (like Genre, Year, Director, Rating range)? Or are they asking a more open-ended question that requires semantic search and summarization?\n",
        "2. If the query is purely filter-based (e.g., \"List all action movies from 2020\"), use the `list_movies_by_filters` tool and provide the identified filters as a dictionary to the tool.\n",
        "3. If the query requires semantic understanding or a combination of semantic search and filtering (e.g., \"Tell me about a good sci-fi movie from the 90s\"), use the `semantic_search_movies_with_filters` tool, providing the core query and any identified filters as a dictionary to the tool.\n",
        "4. If the query is about a specific movie's plot, use the `get_movie_plot` tool.\n",
        "5. If the query is about basic details of a specific movie available in the dataset description, use the `get_movie_description` tool first, and then `get_movie_plot` if more details are needed.\n",
        "6. If you cannot answer the question using the available tools, state that you cannot help with that request.\n",
        "\n",
        "Include conversational history in your responses where relevant.\n",
        "\n",
        "Begin!\n",
        "\n",
        "Previous conversation:\n",
        "{chat_history}\n",
        "\n",
        "Question: {input}\n",
        "{agent_scratchpad}\n",
        "\"\"\"\n",
        "\n",
        "# Initialize ConversationBufferMemory\n",
        "# Ensure memory_key and input_key match the agent's expected input format\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "\n",
        "# Initialize the agent with memory\n",
        "# For OPENAI_FUNCTIONS agent type with memory, initialize_agent handles\n",
        "# passing the memory and chat_history correctly if memory_key is set.\n",
        "# The prefix also needs to include {chat_history}.\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.OPENAI_FUNCTIONS, # Use OPENAI_FUNCTIONS\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True,\n",
        "    memory=memory, # Add memory to the agent\n",
        "    agent_kwargs={\n",
        "        'prefix': agent_prefix, # Pass the custom prefix\n",
        "        'input_variables': ['input', 'chat_history', 'agent_scratchpad', 'tools'] # Explicitly define input variables\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"Langchain agent initialized with memory and tools.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b54175d8",
        "outputId": "255c3eb3-4fd6-4a4c-9e62-5a197bba3938"
      },
      "source": [
        "# Display available tools for the agent with their provided descriptions.\n",
        "\n",
        "if 'tools' in globals() and tools:\n",
        "    print(\"Available tools for the agent:\")\n",
        "    for tool in tools:\n",
        "        print(f\"- **Tool Name:** {tool.name}\")\n",
        "        print(f\"  **Description:** {tool.description}\")\n",
        "        print(\"-\" * 20) # Separator for readability\n",
        "elif 'tools' in globals() and not tools:\n",
        "    print(\"The 'tools' list is defined but currently empty.\")\n",
        "else:\n",
        "    print(\"The 'tools' list is not defined yet. Please run the cells that define and populate the 'tools' list.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available tools for the agent:\n",
            "- **Tool Name:** get_movie_description\n",
            "  **Description:** Searches the dataset for a movie by its exact title and returns its full description from the dataset metadata.\n",
            "Use this tool when the user asks for general information about a specific movie.\n",
            "The input MUST be the exact movie title as a string for the 'movie_title' parameter.\n",
            "--------------------\n",
            "- **Tool Name:** list_movies_by_filters\n",
            "  **Description:** Filters the movie DataFrame directly based on metadata criteria and lists matching titles.\n",
            "Use this tool ONLY for queries that can be answered by listing movies based purely on explicit metadata filters (like Genre, Year, Director, or Rating ranges), without needing semantic search or summarization.\n",
            "The input MUST be a dictionary for 'filter_criteria'.\n",
            "\n",
            "Example filter_criteria:\n",
            "{'Genre': 'Action', 'Year': 2022}\n",
            "{'Director': 'Christopher Nolan'}\n",
            "{'IMDb Rating': {'$gte': 8.0}} # Example of range filter for numeric columns\n",
            "\n",
            "Returns a comma-separated string of matching movie titles.\n",
            "--------------------\n",
            "- **Tool Name:** semantic_search_movies_with_filters\n",
            "  **Description:** Performs a hybrid semantic (vector) and keyword (BM25) search for movies,\n",
            "applies metadata filters, and uses an LLM to answer the query.\n",
            "Use this tool for queries that involve semantic understanding, keyword matching,\n",
            "and/or specific criteria (like Genre, Year, Rating).\n",
            "The input MUST include a 'query' string and can optionally include a 'filters' dictionary.\n",
            "\n",
            "Example filters:\n",
            "{'Genre': 'Action'}\n",
            "{'Year': 2022}\n",
            "{'IMDb Rating': {'$gte': 7.5}} # Example using MongoDB-like query for rating >= 7.5\n",
            "--------------------\n",
            "- **Tool Name:** get_movie_plot\n",
            "  **Description:** Fetches the plot summary for a specific movie given its exact title using an external API (like OMDb).\n",
            "Use this tool when the user explicitly asks for the plot of a named movie.\n",
            "Requires an API key for the external service.\n",
            "The input MUST be the exact movie title as a string for the 'movie_title' parameter.\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the agent with a sample query"
      ],
      "metadata": {
        "id": "3-6-2Why3g37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the agent with a sample query\n",
        "agent_response = agent.invoke(\"How can I find movies from the 90s\")\n",
        "print(agent_response)"
      ],
      "metadata": {
        "id": "og85T3XL3YMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio UI for testing agent"
      ],
      "metadata": {
        "id": "0hN5_HdT3jET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement agent interface, integrate Gradio.\n",
        "\n",
        "# The output of agent.invoke() is a dictionary that includes the\n",
        "# original query and the generated answer with a key 'output'.\n",
        "# So to get result, explicitly pass the 'output' key\n",
        "\n",
        "# Note: to debug any errors with Gradio in Colab, set debug=True in launch below..\n",
        "\n",
        "gr.ChatInterface(lambda message, history: agent.invoke(message)['output']).launch(share=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xoTsV0NlT9aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tests\n"
      ],
      "metadata": {
        "id": "MBOI1-LrAjum"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "280443e8"
      },
      "source": [
        "# Refactored test code to run within the notebook\n",
        "\n",
        "# Assume the necessary variables and functions (df, get_movie_description,\n",
        "# list_movies_by_filters, get_movie_plot, OMDB_API_KEY if needed)\n",
        "# are defined in previous cells and are accessible in the current scope.\n",
        "\n",
        "def test_get_movie_description_exists():\n",
        "  \"\"\"Tests if get_movie_description returns a description for a known movie.\"\"\"\n",
        "  print(\"\\nRunning test_get_movie_description_exists...\")\n",
        "  if 'df' not in globals() or df is None:\n",
        "      print(\"Skipping test: DataFrame not loaded.\")\n",
        "      return # Skip test if df is not available\n",
        "  known_movie_title = \"End of the Spear\" # Replace with a title from your dataset\n",
        "  description = get_movie_description(known_movie_title)\n",
        "  assert known_movie_title in description, f\"Expected '{known_movie_title}' in description, but got: {description}\"\n",
        "  print(\"test_get_movie_description_exists passed.\")\n",
        "\n",
        "def test_get_movie_description_not_exists():\n",
        "  \"\"\"Tests if get_movie_description handles a non-existent movie.\"\"\"\n",
        "  print(\"\\nRunning test_get_movie_description_not_exists...\")\n",
        "  if 'df' not in globals() or df is None:\n",
        "      print(\"Skipping test: DataFrame not loaded.\")\n",
        "      return # Skip test if df is not available\n",
        "  non_existent_movie_title = \"NonExistentMovie123\"\n",
        "  description = get_movie_description(non_existent_movie_title)\n",
        "  assert \"Sorry, I could not find\" in description, f\"Expected 'not found' message, but got: {description}\"\n",
        "  print(\"test_get_movie_description_not_exists passed.\")\n",
        "\n",
        "\n",
        "# Add tests for get_movie_plot if you have OMDB_API_KEY set up\n",
        "# Note: This test requires the OMDB_API_KEY to be available in the environment or mocked.\n",
        "def test_get_movie_plot_exists():\n",
        "    \"\"\"Tests if get_movie_plot fetches a plot for a known movie (requires API key).\"\"\"\n",
        "    print(\"\\nRunning test_get_movie_plot_exists...\")\n",
        "    if 'get_movie_plot' not in globals():\n",
        "        print(\"Skipping test: get_movie_plot tool not defined.\")\n",
        "        return # Skip if tool not defined\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        OMDB_API_KEY_CHECK = userdata.get('OMDB_API_KEY')\n",
        "        if not OMDB_API_KEY_CHECK:\n",
        "             print(\"Skipping test: OMDB_API_KEY not found in Colab Secrets.\")\n",
        "             return\n",
        "    except ImportError:\n",
        "         print(\"Skipping test: Not running in Colab environment or userdata not available.\")\n",
        "         return\n",
        "\n",
        "    known_movie_title = \"Inception\" # Replace with a known movie\n",
        "    plot = get_movie_plot(known_movie_title)\n",
        "    assert \"Plot information not available.\" not in plot and \"Could not find plot information\" not in plot and \"Movie not found!\" not in plot, f\"Expected plot, but got: {plot}\" # Should return a plot and find the movie\n",
        "    assert len(plot) > 50, f\"Expected plot to be reasonably long, but got length: {len(plot)}\" # Plot should be reasonably long\n",
        "    print(\"test_get_movie_plot_exists passed.\")\n",
        "\n",
        "\n",
        "def test_get_movie_plot_not_exists():\n",
        "    \"\"\"Tests if get_movie_plot handles a non-existent movie (requires API key).\"\"\"\n",
        "    print(\"\\nRunning test_get_movie_plot_not_exists...\")\n",
        "    if 'get_movie_plot' not in globals():\n",
        "        print(\"Skipping test: get_movie_plot tool not defined.\")\n",
        "        return # Skip if tool not defined\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        OMDB_API_KEY_CHECK = userdata.get('OMDB_API_KEY')\n",
        "        if not OMDB_API_KEY_CHECK:\n",
        "             print(\"Skipping test: OMDB_API_KEY not found in Colab Secrets.\")\n",
        "             return\n",
        "    except ImportError:\n",
        "         print(\"Skipping test: Not running in Colab environment or userdata not available.\")\n",
        "         return\n",
        "\n",
        "    non_existent_movie_title = \"ThisMovieDoesNotExist12345\"\n",
        "    plot = get_movie_plot(non_existent_movie_title)\n",
        "    assert \"Movie not found!\" in plot, f\"Expected 'Movie not found!' message, but got: {plot}\" # Should return the exact \"Movie not found!\" message\n",
        "    print(\"test_get_movie_plot_not_exists passed.\")\n",
        "\n",
        "\n",
        "# --- Run the tests ---\n",
        "print(\"--- Running Movie Tool Tests ---\")\n",
        "test_get_movie_description_exists()\n",
        "test_get_movie_description_not_exists()\n",
        "test_get_movie_plot_exists()\n",
        "test_get_movie_plot_not_exists()\n",
        "print(\"--- Finished Running Movie Tool Tests ---\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check the edge cases and handle them appropriately\n",
        "\n",
        "- Edge Case: File not found or incorrect type of file loaded:\n",
        "    - Solution: Handling of possible exceptions that can occur when loading the provided data set. This will prevent from showing the low level exceptions to the user and handle them properly with readable messages.\n",
        "\n",
        "- Edge Case: Issue with losing all the data when the Colab runtime is disconnected.\n",
        "\n",
        "   - Solution: Mounted Google drive to load the data from the drive than generating the data again.\n",
        "\n",
        "\n",
        "- Edge Case: Data related issues like handling missing values, incorrect data types:\n",
        "    - Solution: As all the features of the data frame are beng used to in 'Metadata' any missing or incorrect data type values are dropped from the data frame. But there were no rows that are effected because of this.\n",
        "\n",
        "- Edge Case: Creating vector embeddings is taking a significant amount of time with Colab's default CPU run time:\n",
        "    - Solution: Updated the runtime to T4-GPU. This made the execution much faster.\n",
        "\n",
        "- Edge Case: A tool is called with invalid input types or formats:\n",
        "   - Solution: Using Pydantic models with args_schema helps the agent understand the expected format and provides structured validation.\n",
        "\n",
        "- Edge Case: An external API call fails:\n",
        "   - Solution: Including proper checks and wrapping the execution code with try.. catch will help in catching any exceptions.\n",
        "\n",
        "- Edge Case: The agent fails to correctly parse the user's natural language query and identify the correct tool and its arguments (as seen with the list_movies_by_filters issue).\n",
        "\n",
        "  - Solution:initially started using free LLM 'gpt2.0' modell but as it was failing to provide the search results correctly, switched to gpt4.0 and the issue with parsing is resolved.\n",
        "\n",
        "- Edge Case: The vector search returns no relevant documents.\n",
        "   \n",
        "  - Solution: To avoid this situation, movie_search_prompt asks the LLM not to make up answers if it doesn't know.\n",
        "\n",
        "- Edge Case: Refine Agents tool Selection Prompting\n",
        "   - Solution: Implemented another Lexical retrieval technique 'BM25' which will perform the exact text search as compared to vector search that focuses on semantic search.\n",
        "\n",
        "- Edge Case: Get the movie plot information by RAG technique compared to directly storing in the data set.\n",
        "   - Solution: The get_movie_plot tool is better for providing the most current plot for a specific movie on demand and keeping RAG vector store smaller. Including plot in the dataset/embeddings is better if you need to perform semantic searches based on plot content.\n",
        "\n",
        "- Edge Case: Provide relevance with user queries during search\n",
        "\n",
        "  - Solution: Added session history so that the LLM can remember and provide results by including the history where possible.\n",
        "\n",
        "- Edge Case: SEcurity concerns with API keys\n",
        "  \n",
        "   - Solution: Instead of , manually entering the Keys, used Google Colab's Secret manager to store the keys and accessed in the code by using the Key names rather than actual Key values.\n",
        "\n",
        "- Edge Case: Troubleshooting the issues during searching\n",
        "\n",
        "  - Solution:\n",
        "      - Set the Agent output to  Verbose.\n",
        "      - Gradio launch to debug=True (correctly commented out as debug=True run the UI indefinitely until disconnected manually)"
      ],
      "metadata": {
        "id": "_DzeC9Q_HG9i"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}